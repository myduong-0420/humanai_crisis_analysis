# AI-Powered Behavioral Analysis for Suicide Prevention, Substance Use, and Mental Health Crisis Detection with Longitudinal Geospatial Crisis Trend

### Task 1. Social Media Data Extraction & Preprocessing (API Handling & Text Cleaning)
ðŸ“ŒÂ **Deliverable:**
- `task_1_scrape_clean.py` that retrieves and storesÂ **filtered social media posts**.
- `reddit_posts_cleaned.csv`Â ready for NLP analysis.

The main submission with tokenized text has the name `reddit_posts_cleaned.csv`, but additional datasets were curated to work with task 2 and task 3 more conveniently. `reddit_data_with_sa.csv` contains sentiment scores generated by VADER and Textblob, `sample_test_with_risk_words.csv` contains risk words and severity of the content, and `reddit_posts_geotags.csv` contains ngrams isolated for geotagging, and geographical cues.

I choose to focus on Reddit data, since I want to try tackling insufficient geotagging with NLP methods. Besides, in sensitive areas like mental health and drug use, it's understandable that original posters (OPs) often choose not to disclose their locations, so geotagging will only work when OPs willingly share about their whereabouts in the posts. While the lack of geographical information feature presents a big challenge about data representation, it is also an ethical standard that I want to uphold. One area that I would like to explore further is to visit regional subreddits (E.g, r/tampa) and make queries with selected keywords, but due to time and compute constraint (explained in the next task), I was not able to do this.


### Task 2. Sentiment & Crisis Risk Classification (NLP & Text Processing)
ðŸ“ŒÂ **Deliverable:**
- `task_2_classification.py`Â that classifies posts based onÂ **sentiment and risk level**.
- `sentiment_risk_distribution.png`Â showing theÂ **distribution of posts by sentiment and risk category**.

The Python script is meant for the whole dataset, but due to the size of the original dataset (21K entries), it would take me an estimated time of **17 hours** to extrapolate all risk words and classify which risk category each post falls in. I decided to sample 500 random posts from the dataset and generate `sample_test_with_risk_words.csv`, which has all the sentiment scores, risk words and risk category. This process took around 2 hours to run.

The distribution of posts by sentiment is not surprising - most posts have negative sentiment, followed by positive and neutral. In searching for risk words, I also pulled reference text from various sources on the internet, including counselling websites and AI-generated phrases for different risk levels. I choose the BERT model `all-MiniLM-L6-v2` due to its ability to work with context and semantic dependencies over Word2Vec, and with this, I created 5-grams for each post and compared them with reference text, thus isolating the highest-ranked 5-grams as risk phrases. This is the most expensive process out of the whole pipeline that I worked on.


### Task 3. Crisis Geolocation & Mapping (Basic Geospatial Analysis & Visualization)
ðŸ“Œ **Deliverable:**
- AÂ **Python script**Â that geocodes posts and generates aÂ **heatmap**Â of crisis discussions.
- AÂ **visualization of regional distress patterns**Â in the dataset.
