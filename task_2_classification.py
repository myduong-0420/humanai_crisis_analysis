import pandas as pd
import matplotlib.pyplot as plt
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from textblob import TextBlob
from tqdm import tqdm
from sentence_transformers import SentenceTransformer, util

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import time
import random

tqdm.pandas()

# lists of sentences/ phrases with different levels of concern, generated by GPT-4o
# High-Risk: Direct crisis language
high_risk = [
    "I don’t want to be here anymore.",
    "There’s no way out of this.",
    "I just want the pain to stop.",
    "I can’t do this anymore.",
    "I wish I could disappear forever.",
    "No one would care if I was gone.",
    "I don’t see a reason to keep going.",
    "It all feels so pointless.",
    "I’m tired of pretending I’m okay.",
    "I’ve been thinking about ending everything.",
    "I just want to sleep and never wake up.",
    "Nothing matters anymore.",
    "I don’t have the energy to fight anymore.",
    "Everything would be better if I was gone.",
    "I can’t escape this darkness.",
    "I want to die",
    "nothing matters anymore",
    "ending it all"
]

# Moderate Concern: Seeking help, discussing struggles
moderate_concern = [
    "I feel lost lately.",
    "I don’t know how to cope with everything.",
    "Some days are just too hard to handle.",
    "I feel like I’m drowning in my thoughts.",
    "I wish someone understood how I feel.",
    "It’s hard to find a reason to get out of bed.",
    "I keep pushing through, but it’s exhausting.",
    "I feel numb to everything around me.",
    "I don’t know who to talk to about this.",
    "I feel like I’m carrying too much alone.",
    "Lately, everything just feels overwhelming.",
    "I hate feeling like this all the time.",
    "I can’t remember the last time I was truly happy.",
    "I wish I could just catch a break.",
    "I’ve been struggling, but I don’t know what to do."
]

# Low Concern: General mental health discussions
low_concern = [
    "Taking care of your mental health is important.",
    "Therapy has been really helpful for me.",
    "Sometimes self-care means setting boundaries.",
    "Anxiety can be tough, but there are ways to manage it.",
    "I’ve been learning a lot about mindfulness lately.",
    "Journaling helps me process my emotions.",
    "It’s okay to not be okay sometimes.",
    "I think more people should talk about mental health.",
    "Depression affects people in different ways.",
    "Having a strong support system makes a big difference.",
    "Exercise has really helped improve my mood.",
    "Sleep is so important for mental well-being.",
    "Breaking the stigma around mental health is necessary.",
    "Small wins matter when dealing with mental health.",
    "Managing stress takes effort, but it’s worth it."
]

def retrieve_seed_phrases(data_path, url):
	driver = webdriver.Chrome()
	driver.maximize_window()
	options = Options()
	options.add_argument("--disable-blink-features=AutomationControlled")
	options.add_argument("--headless")
	driver.get(url)
	time.sleep(3)
	page_source = driver.page_source
	with open(rf"{data_path}\references.html", "w", encoding="utf-8") as f:
		f.write(page_source)
		f.close()

def get_reference_text(data_path):
    with open(rf"{data_path}\references.txt", "r", encoding="utf-8") as f:
        high_risk_phrases = f.readlines()
        f.close()

    high_risk_phrases.extend(high_risk)
    all_phrases = high_risk_phrases.copy()
    all_phrases.extend(moderate_concern)
    all_phrases.extend(low_concern)
    return high_risk_phrases, all_phrases

def vader_sentiment(text):
	analyzer = SentimentIntensityAnalyzer()
	sentiment = analyzer.polarity_scores(text)
	return sentiment["neg"], sentiment["neu"], sentiment["pos"] 	

def textblob_sentiment(text):
	text_doc = TextBlob(text)
	sentiment = text_doc.sentiment
	return sentiment[0], sentiment[1]

def df_sentiment(df):
	for i, row in tqdm(df.iterrows(), total=len(df)):
		if pd.isnull(row["content"]):
			text = row["title"]
		text = str(row["title"]) + " " + str(row["content"])
		neg, neu, pos = vader_sentiment(text)
		df.at[i, "v_neg"] = neg
		df.at[i, "v_neu"] = neu
		df.at[i, "v_pos"] = pos
		df.at[i, "t_polarity"] = textblob_sentiment(text)[0]
		df.at[i, "t_subjectivity"] = textblob_sentiment(text)[1]
	return df

def seed_embeddings(seed_phrases, model="all-MiniLM-L6-v2"):
	model = SentenceTransformer(model)
	embeddings = model.encode(seed_phrases, convert_to_tensor=True)
	return embeddings

def generate_ngrams(text, n=5):
    tokens = text.split()
    ngrams = []
    for i in range(len(tokens) - n + 1):
        ngram_tokens = tokens[i : i + n]
        ngrams.append(" ".join(ngram_tokens))
    return ngrams

def detect_high_risk_ngrams(text, seed_phrases, n=5, threshold=0.7):
    ref_embeddings = seed_embeddings(seed_phrases)
    ngrams = generate_ngrams(text=text, n=n)
    if not ngrams:
        return []

    ngram_embeddings = seed_embeddings(ngrams)

    cos_sim_matrix = util.cos_sim(ngram_embeddings, ref_embeddings)
    flagged_ngrams = []
    for i, ngram in enumerate(ngrams):
        row_similarities = cos_sim_matrix[i]
        if row_similarities.max() >= threshold:
            best_seed_idx = int(row_similarities.argmax())
            best_seed_phrase = seed_phrases[best_seed_idx]
            best_score = float(row_similarities[best_seed_idx])

            flagged_ngrams.append({
                "ngram": ngram,
                "similar_seed_phrase": best_seed_phrase,
                "similarity_score": best_score
            })

    filtered_flagged = []
    i = 0
    while i < len(flagged_ngrams):
        if i < len(flagged_ngrams) - 1:
            current_ngram = flagged_ngrams[i]["ngram"].split()
            next_ngram = flagged_ngrams[i+1]["ngram"].split()
            if flagged_ngrams[i]["similar_seed_phrase"] == flagged_ngrams[i+1]["similar_seed_phrase"]:
                if current_ngram[1:] == next_ngram[:-1]:
                    if flagged_ngrams[i]["similarity_score"] < flagged_ngrams[i+1]["similarity_score"]:
                        filtered_flagged.append(flagged_ngrams[i+1])
                    else:
                        filtered_flagged.append(flagged_ngrams[i])
                    i += 2
                    continue
        filtered_flagged.append(flagged_ngrams[i])
        i += 1
    return filtered_flagged

def risk_category(df):
	for i, row in df.iterrows():
		check_high_risk = False
		check_moderate_risk = False
		check_low_risk = True
		for item in row["risk_words"]:
			if len(item) == 0:
				continue
			if item["similar_seed_phrase"] in ref_phrases:
				check_high_risk = True
			elif item["similar_seed_phrase"] in moderate_concern:
				check_moderate_risk = True

		if check_high_risk:
			df.at[i, "risk_category"] = "high"
		else:
			if check_moderate_risk:
				df.at[i, "risk_category"] = "moderate"
			else:
				if check_low_risk:
					df.at[i, "risk_category"] = "low"
	return df

if __name__ == "__main__":
	ref_link_1 = rf"https://mywellbeing.com/therapy-101/what-depression-feels-like"
	data_path = rf"D:\humanai_crisis_analysis\data"
	all_phrases, ref_phrases = get_reference_text(data_path)
	df = pd.read_csv(rf"{data_path}\reddit_posts.csv")
	df_with_sa = df_sentiment(df)
	# df_with_sa.to_csv(rf"D:\humanai_crisis_analysis\data\reddit_data_with_sa.pkl", index=False)
	test_df = df_with_sa.sample(500)
	test_df["content"] = test_df["content"].fillna("")
	test_df["title"] = test_df["title"].fillna("")
	test_df["full_text"] = test_df["title"] + " " + test_df["content"]
	test_df["risk_words"] = test_df["full_text"].progress_apply(
		lambda x: detect_high_risk_ngrams(x, all_phrases, n=5, threshold=0.6))
	test_df = risk_category(test_df)
	test_df.to_csv(rf"{data_path}\test_with_risk_words.csv", index=False)
	
    # plot by sentiment and risk category: count of sentiment and risk category
	sentiment_count = test_df.groupby(["s_class"]).size().reset_index(name="sentiment_count")
	risk_count = test_df.groupby(["risk_category"]).size().reset_index(name="risk_count")
	
	fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
	ax1.bar(sentiment_count["s_class"], sentiment_count["sentiment_count"])
	ax1.set_title("Sentiment Distribution")
	ax1.set_xlabel("Sentiment")
	ax1.set_ylabel("Count")
	
	ax2.bar(risk_count["risk_category"], risk_count["risk_count"])
	ax2.set_title("Risk Category Distribution")
	ax2.set_xlabel("Risk Category")
	ax2.set_ylabel("Count")
	
	plt.savefig(rf"{data_path}\sentiment_risk_distribution.png")
	plt.show()